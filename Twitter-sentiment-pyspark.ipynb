{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr5pnmetilSO",
        "outputId": "697477d8-3e93-4fb8-9d4f-ba5c835c7142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Twitter Sentiment Analysis\").getOrCreate()"
      ],
      "metadata": {
        "id": "2-M3EJYZi8b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading dataset\n",
        "df = spark.read.csv('/content/drive/MyDrive/training.1600000.processed.noemoticon.csv', header=False, inferSchema=True)\n",
        "df = df.selectExpr(\"_c0 as label\", \"_c5 as tweet\")\n",
        "\n",
        "df = df.filter((df.label == 0) | (df.label == 4))\n",
        "\n",
        "from pyspark.sql.functions import when\n",
        "df = df.withColumn(\"label\", when(df.label == 4, 1).otherwise(0))\n",
        "\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxhulSWJjAbW",
        "outputId": "dfb96e3a-5c04-426c-bd4e-b0c81a97e403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+\n",
            "|label|               tweet|\n",
            "+-----+--------------------+\n",
            "|    0|@switchfoot http:...|\n",
            "|    0|is upset that he ...|\n",
            "|    0|@Kenichan I dived...|\n",
            "|    0|my whole body fee...|\n",
            "|    0|@nationwideclass ...|\n",
            "+-----+--------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset is\",df.count())\n",
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWls4VL7jEiG",
        "outputId": "8b403f6d-cadb-4ec5-de3e-f2369d7b0338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset is 1600000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['label', 'tweet']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lower, regexp_replace, trim\n",
        "\n",
        "def clean_text_df(df, input_col=\"tweet\", output_col=\"clean_text\"):\n",
        "    df = df.withColumn(output_col, lower(col(input_col)))\n",
        "    df = df.withColumn(output_col, regexp_replace(output_col, r\"http\\S+|www\\S+\", \"\"))\n",
        "    df = df.withColumn(output_col, regexp_replace(output_col, r\"@\\w+\", \"\"))\n",
        "    df = df.withColumn(output_col, regexp_replace(output_col, r\"#\", \"\"))\n",
        "    df = df.withColumn(output_col, regexp_replace(output_col, r\"[^a-zA-Z\\s]\", \"\"))\n",
        "    df = df.withColumn(output_col, regexp_replace(output_col, r\"\\s+\", \" \"))\n",
        "    df = df.withColumn(output_col, trim(output_col))\n",
        "    return df\n",
        "df = clean_text_df(df)\n",
        "df.select(\"tweet\", \"clean_text\").show(5, truncate=False)"
      ],
      "metadata": {
        "id": "u_TeMGHujOoj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1c2ae77-6c4e-4585-d3a9-bc7a04dfeef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+\n",
            "|tweet                                                                                                              |clean_text                                                                                              |\n",
            "+-------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+\n",
            "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|a thats a bummer you shoulda got david carr of third day to do it d                                     |\n",
            "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |is upset that he cant update his facebook by texting it and might cry as a result school today also blah|\n",
            "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          |i dived many times for the ball managed to save the rest go out of bounds                               |\n",
            "|my whole body feels itchy and like its on fire                                                                     |my whole body feels itchy and like its on fire                                                          |\n",
            "|@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.     |no its not behaving at all im mad why am i here because i cant see you all over there                   |\n",
            "+-------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import (\n",
        "    Tokenizer, StopWordsRemover,VectorAssembler, NGram,\n",
        "    CountVectorizer, IDF\n",
        ")\n",
        "\n",
        "tokenizer = Tokenizer(\n",
        "    inputCol=\"clean_text\",\n",
        "    outputCol=\"tokens\"\n",
        ")\n",
        "\n",
        "remover = StopWordsRemover(\n",
        "    inputCol=\"tokens\",\n",
        "    outputCol=\"filtered_tokens\"\n",
        ")\n",
        "\n",
        "unigram_cv = CountVectorizer(\n",
        "    inputCol=\"filtered_tokens\",\n",
        "    outputCol=\"unigram_features\",\n",
        "    vocabSize=20000,\n",
        "    minDF=5\n",
        ")\n",
        "\n",
        "bigram = NGram(\n",
        "    n=2,\n",
        "    inputCol=\"filtered_tokens\",\n",
        "    outputCol=\"bigrams\"\n",
        ")\n",
        "\n",
        "bigram_cv = CountVectorizer(\n",
        "    inputCol=\"bigrams\",\n",
        "    outputCol=\"bigram_features\",\n",
        "    vocabSize=20000,\n",
        "    minDF=5\n",
        ")\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"unigram_features\",\"bigram_features\"],\n",
        "    outputCol=\"raw_features\",\n",
        ")\n",
        "\n",
        "idf = IDF(\n",
        "    inputCol=\"raw_features\",\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "feature_stages = [\n",
        "    tokenizer,\n",
        "    remover,\n",
        "    bigram,\n",
        "    unigram_cv,\n",
        "    bigram_cv,\n",
        "    assembler,\n",
        "    idf\n",
        "]"
      ],
      "metadata": {
        "id": "f8H4Gfu8jXji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df,test_df = df.randomSplit([0.8,0.2],seed= 42)\n",
        "train_df.cache()\n",
        "test_df.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWHDBmeAhHhP",
        "outputId": "12b41957-dcd4-4928-ab22-bbf8f4d546f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[label: int, tweet: string, clean_text: string]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when,col\n",
        "\n",
        "label_counts = df.groupBy('label').count().collect()\n",
        "\n",
        "neg_count = [row[\"count\"] for row in label_counts if row[\"label\"] == 0][0]\n",
        "pos_count = [row[\"count\"] for row in label_counts if row[\"label\"] == 0][0]\n",
        "\n",
        "total = neg_count + pos_count\n",
        "\n",
        "train_df = train_df.withColumn(\n",
        "    \"classWeightCol\",\n",
        "    when(col(\"label\") == 1,total/pos_count).otherwise(total/neg_count)\n",
        ")\n",
        "\n",
        "test_df = test_df.withColumn(\"classWeightCol\",col(\"label\"))"
      ],
      "metadata": {
        "id": "WnFF1h3VqsvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import (\n",
        "    LogisticRegression,\n",
        "    NaiveBayes,\n",
        "    LinearSVC\n",
        ")\n",
        "models ={\n",
        "    \"naive_bayes\": NaiveBayes(\n",
        "        featuresCol=\"features\",\n",
        "        labelCol=\"label\"\n",
        "    ),\n",
        "    \"linear_svc\": LinearSVC(\n",
        "        featuresCol=\"features\",\n",
        "        labelCol=\"label\"\n",
        "    )\n",
        "  }\n",
        "\n",
        "lr = LogisticRegression(\n",
        "        featuresCol=\"features\",\n",
        "        labelCol=\"label\",\n",
        "        weightCol=\"classWeightCol\",\n",
        "        regParam=0.1\n",
        "    )"
      ],
      "metadata": {
        "id": "6i5SzDLbjnBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.tuning import CrossValidator,ParamGridBuilder\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator,BinaryClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "paramGrid = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(lr.regParam,[0.001,0.01,0.1])\n",
        "    .addGrid(lr.elasticNetParam,[0.0,0.5,1.0])\n",
        "    .build()\n",
        ")\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"f1\"\n",
        ")\n",
        "\n",
        "evaluator_auc = BinaryClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    rawPredictionCol=\"rawPrediction\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "\n",
        "cv_pipeline = Pipeline(\n",
        "    stages=feature_stages + [lr]\n",
        ")\n",
        "\n",
        "crossval = CrossValidator(\n",
        "    estimator=cv_pipeline,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=evaluator,\n",
        "    numFolds=2,\n",
        "    parallelism=2\n",
        ")"
      ],
      "metadata": {
        "id": "shFr-JIHs96d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import (\n",
        "    MulticlassClassificationEvaluator,\n",
        "    BinaryClassificationEvaluator\n",
        ")\n",
        "\n",
        "def evaluate_predictions(predictions):\n",
        "    f1 = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"label\",\n",
        "        predictionCol=\"prediction\",\n",
        "        metricName=\"f1\"\n",
        "    ).evaluate(predictions)\n",
        "\n",
        "    auc = BinaryClassificationEvaluator(\n",
        "        labelCol=\"label\",\n",
        "        rawPredictionCol=\"rawPrediction\",\n",
        "        metricName=\"areaUnderROC\"\n",
        "    ).evaluate(predictions)\n",
        "\n",
        "    return f1,auc\n"
      ],
      "metadata": {
        "id": "wGX2ASGtj-eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name,model in models.items():\n",
        "  print(f\"\\nTraining {name}...\")\n",
        "\n",
        "  pipeline = Pipeline(stages=feature_stages+[model])\n",
        "  fitted_model = pipeline.fit(train_df)\n",
        "  predictions = fitted_model.transform(test_df)\n",
        "  f1,auc = evaluate_predictions(predictions)\n",
        "\n",
        "  results[name] = {\n",
        "      \"model\": fitted_model,\n",
        "      \"f1\": f1,\n",
        "      \"auc\": auc\n",
        "  }\n",
        "  print(f\"{name} -> F1:{float(f1):.4f}, ROC_AUC:{float(auc):.4f}\")\n",
        "\n",
        "print(\"Training logistic regression...\")\n",
        "\n",
        "cv_model = crossval.fit(train_df)\n",
        "\n",
        "cv_predictions = cv_model.transform(test_df)\n",
        "\n",
        "f1_lr = evaluator.evaluate(cv_predictions)\n",
        "auc_lr = evaluator_auc.evaluate(cv_predictions)\n",
        "\n",
        "print(f\"\\nCross-Validated Logistic Regression -> F1: {f1_lr:.4f}, ROC-AUC: {auc_lr:.4f}\")\n",
        "\n",
        "results[\"logistic_regression_cv\"] = {\n",
        "    \"model\": cv_model,\n",
        "    \"f1\": float(f1_lr),\n",
        "    \"auc\": float(auc_lr)\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqWjIlq4iJZd",
        "outputId": "62e73c8b-ac31-4c13-c6dc-0a364edf6e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training naive_bayes...\n",
            "naive_bayes -> F1:0.7731, ROC_AUC:0.5494\n",
            "\n",
            "Training linear_svc...\n",
            "linear_svc -> F1:0.7881, ROC_AUC:0.8631\n",
            "Training logistic regression...\n",
            "\n",
            "Cross-Validated Logistic Regression -> F1: 0.7897, ROC-AUC: 0.8675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_name = max(results,key=lambda x: results[x][\"f1\"])\n",
        "best_model = results[best_model_name][\"model\"]\n",
        "\n",
        "preds = best_model.transform(test_df)\n",
        "\n",
        "print(\"FALSE POSITIVES:\")\n",
        "preds.filter(\n",
        "    (col(\"label\") == 0) & (col(\"prediction\") == 1)\n",
        ").select(\"tweet\").show(5,truncate=False)\n",
        "\n",
        "print(\"FALSE NEGATIVES:\")\n",
        "preds.filter(\n",
        "    (col(\"label\") == 1) & (col(\"prediction\") == 0)\n",
        ").select(\"tweet\").show(5,truncate=False)"
      ],
      "metadata": {
        "id": "FCSENGrajtML",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe41556d-55ca-4d1e-e745-c755795953f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FALSE POSITIVES:\n",
            "+----------------------------------------------------------+\n",
            "|tweet                                                     |\n",
            "+----------------------------------------------------------+\n",
            "|   #Battleground                                          |\n",
            "|   Tell those girls your clocking out lol                 |\n",
            "|   no shopping                                            |\n",
            "|  (unsure)  (hassle)  (:  (music) http://plurk.com/p/yq08l|\n",
            "|  Anyone in DFW wanna hang out?                           |\n",
            "+----------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "FALSE NEGATIVES:\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|tweet                                                                                                                                    |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|#ASOT400 SAVE SIMON FOR LAST!   cause his stuff bumps the hardest                                                                        |\n",
            "|'And you can tell me that you're sorry but I don't believe you baby like I did before... You're not sorry'                               |\n",
            "|- Sitting in the studio talking to Danniee Beee who is ignoring me as he is adding friends to his Facebook;that is until he reads this   |\n",
            "|... 31 days until I leave for Ontario!!                                                                                                  |\n",
            "|... Wanting to watch Twilight again, but I'm going to the hairdressers soon. *sigh*... When I get back?!  &quot;Animal Attack&quot; Swoon|\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "def predict_text(model,spark,text):\n",
        "  temp_df = spark.createDataFrame([Row(tweet = text)])\n",
        "  temp_df = clean_text_df(temp_df)\n",
        "\n",
        "  result = model.transform(temp_df).select(\"prediction\",\"probability\").collect()[0]\n",
        "\n",
        "  return {\n",
        "      \"prediction\": int(result[\"prediction\"]),\n",
        "      \"probablities\": list(result[\"probability\"]),\n",
        "      \"confidence\": max(result[\"probability\"])\n",
        "  }"
      ],
      "metadata": {
        "id": "xtR7IzR3UUA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = predict_text(\n",
        "    best_model,\n",
        "    spark,\n",
        "    \"This movie was absolutely amazing!\"\n",
        ")\n",
        "\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5yx_hHsVvlP",
        "outputId": "6091b4b9-05f7-4ddc-ee53-c5d5f5c925fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prediction': 1,\n",
              " 'probablities': [np.float64(0.13719325956202927),\n",
              "  np.float64(0.8628067404379707)],\n",
              " 'confidence': np.float64(0.8628067404379707)}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDfAtqcSXbyX",
        "outputId": "1591a77a-2e6f-41d2-9ac3-7e4931fe931d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-3.9.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting mlflow-skinny==3.9.0 (from mlflow)\n",
            "  Downloading mlflow_skinny-3.9.0-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting mlflow-tracing==3.9.0 (from mlflow)\n",
            "  Downloading mlflow_tracing-3.9.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting Flask-CORS<7 (from mlflow)\n",
            "  Downloading flask_cors-6.0.2-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.1.2)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.18.1)\n",
            "Requirement already satisfied: cryptography<47,>=43.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (43.0.3)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting huey<3,>=2.5.4 (from mlflow)\n",
            "  Downloading huey-2.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.2)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<23,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.16.3)\n",
            "Collecting skops<1 (from mlflow)\n",
            "  Downloading skops-0.13.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.46)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (6.2.5)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (8.3.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (3.1.2)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.9.0->mlflow)\n",
            "  Downloading databricks_sdk-0.82.0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fastapi<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (0.123.10)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (3.1.46)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (8.7.1)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (25.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (2.12.3)\n",
            "Requirement already satisfied: python-dotenv<2,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (1.2.1)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (2.32.4)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (0.5.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (4.15.0)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.9.0->mlflow) (0.40.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<47,>=43.0.0->mlflow) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.5)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (3.3.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: prettytable>=3.9 in /usr/local/lib/python3.12/dist-packages (from skops<1->mlflow) (3.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.3.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<47,>=43.0.0->mlflow) (3.0)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.12/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow) (2.43.0)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.9.0->mlflow) (0.50.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.9.0->mlflow) (0.0.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.9.0->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.9.0->mlflow) (3.23.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.9.0->mlflow) (0.58b0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prettytable>=3.9->skops<1->mlflow) (0.4.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.9.0->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.9.0->mlflow) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.9.0->mlflow) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.9.0->mlflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.9.0->mlflow) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.9.0->mlflow) (2026.1.4)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1->mlflow-skinny==3.9.0->mlflow) (0.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.9.0->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.51.0,>=0.40.0->fastapi<1->mlflow-skinny==3.9.0->mlflow) (4.12.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow) (0.6.2)\n",
            "Downloading mlflow-3.9.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-3.9.0-py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_tracing-3.9.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask_cors-6.0.2-py3-none-any.whl (13 kB)\n",
            "Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huey-2.6.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading skops-0.13.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.2/131.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.82.0-py3-none-any.whl (789 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m789.2/789.2 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: huey, gunicorn, graphql-core, graphql-relay, docker, skops, graphene, Flask-CORS, databricks-sdk, mlflow-tracing, mlflow-skinny, mlflow\n",
            "Successfully installed Flask-CORS-6.0.2 databricks-sdk-0.82.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.7 graphql-relay-3.2.0 gunicorn-23.0.0 huey-2.6.0 mlflow-3.9.0 mlflow-skinny-3.9.0 mlflow-tracing-3.9.0 skops-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import mlflow.spark\n",
        "\n",
        "mlflow.set_experiment(\"Twitter Sentiment Analysis- pyspark\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrbjgNOzX0Qi",
        "outputId": "14234f92-b32d-4116-b7d1-188aa8146ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026/01/30 11:27:16 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.schemas\n",
            "2026/01/30 11:27:16 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.tables\n",
            "2026/01/30 11:27:16 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.types\n",
            "2026/01/30 11:27:16 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.constraints\n",
            "2026/01/30 11:27:16 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.defaults\n",
            "2026/01/30 11:27:16 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.comments\n",
            "2026/01/30 11:27:16 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
            "2026/01/30 11:27:16 INFO mlflow.store.db.utils: Updating database tables\n",
            "2026/01/30 11:27:16 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
            "2026/01/30 11:27:16 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade  -> 451aebb31d03, add metric step\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions\n",
            "2026/01/30 11:27:17 INFO alembic.runtime.migration: Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables\n",
            "2026/01/30 11:27:18 INFO alembic.runtime.migration: Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n",
            "2026/01/30 11:27:18 INFO alembic.runtime.migration: Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size\n",
            "2026/01/30 11:27:18 INFO alembic.runtime.migration: Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000\n",
            "2026/01/30 11:27:18 INFO alembic.runtime.migration: Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments\n",
            "2026/01/30 11:27:18 INFO alembic.runtime.migration: Running upgrade 0584bdc529eb -> 400f98739977, add logged model tables\n",
            "2026/01/30 11:27:18 INFO alembic.runtime.migration: Running upgrade 400f98739977 -> 6953534de441, add step to inputs table\n",
            "2026/01/30 11:27:18 INFO alembic.runtime.migration: Running upgrade 6953534de441 -> bda7b8c39065, increase_model_version_tag_value_limit\n",
            "2026/01/30 11:27:18 INFO alembic.runtime.migration: Running upgrade bda7b8c39065 -> cbc13b556ace, add V3 trace schema columns\n",
            "2026/01/30 11:27:18 INFO alembic.runtime.migration: Running upgrade cbc13b556ace -> 770bee3ae1dd, add assessments table\n",
            "2026/01/30 11:27:18 INFO alembic.runtime.migration: Running upgrade 770bee3ae1dd -> a1b2c3d4e5f6, add spans table\n",
            "2026/01/30 11:27:18 INFO alembic.runtime.migration: Running upgrade a1b2c3d4e5f6 -> de4033877273, create entity_associations table\n",
            "2026/01/30 11:27:18 INFO alembic.runtime.migration: Running upgrade de4033877273 -> 1a0cddfcaa16, Add webhooks and webhook_events tables\n",
            "2026/01/30 11:27:18 INFO alembic.runtime.migration: Running upgrade 1a0cddfcaa16 -> 534353b11cbc, add scorer tables\n",
            "2026/01/30 11:27:18 INFO alembic.runtime.migration: Running upgrade 534353b11cbc -> 71994744cf8e, add evaluation datasets\n",
            "2026/01/30 11:27:18 INFO alembic.runtime.migration: Running upgrade 71994744cf8e -> 3da73c924c2f, add outputs to dataset record\n",
            "2026/01/30 11:27:18 INFO alembic.runtime.migration: Running upgrade 3da73c924c2f -> bf29a5ff90ea, add jobs table\n",
            "2026/01/30 11:27:18 INFO alembic.runtime.migration: Running upgrade bf29a5ff90ea -> 1bd49d398cd23, add secrets tables\n",
            "2026/01/30 11:27:18 INFO alembic.runtime.migration: Running upgrade 1bd49d398cd23 -> b7c8d9e0f1a2, add trace metrics table\n",
            "2026/01/30 11:27:19 INFO alembic.runtime.migration: Running upgrade b7c8d9e0f1a2 -> 5d2d30f0abce, update job table\n",
            "2026/01/30 11:27:19 INFO alembic.runtime.migration: Running upgrade 5d2d30f0abce -> c9d4e5f6a7b8, add routing strategy to endpoints and linkage type to mappings\n",
            "2026/01/30 11:27:19 INFO alembic.runtime.migration: Running upgrade c9d4e5f6a7b8 -> 2c33131f4dae, add online_scoring_configs table\n",
            "2026/01/30 11:27:19 INFO alembic.runtime.migration: Running upgrade 2c33131f4dae -> d3e4f5a6b7c8, add display_name to endpoint_bindings\n",
            "2026/01/30 11:27:19 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
            "2026/01/30 11:27:19 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
            "2026/01/30 11:27:19 INFO mlflow.tracking.fluent: Experiment with name 'Twitter Sentiment Analysis- pyspark' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='/content/mlruns/1', creation_time=1769772439224, experiment_id='1', last_update_time=1769772439224, lifecycle_stage='active', name='Twitter Sentiment Analysis- pyspark', tags={}>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_f1 = results[best_model_name][\"f1\"]\n",
        "best_auc = results[best_model_name][\"auc\"]\n",
        "\n",
        "with mlflow.start_run(run_name=best_model_name):\n",
        "  mlflow.log_param(\"model\",best_model_name)\n",
        "  mlflow.log_metric(\"f1_score\",best_f1)\n",
        "  mlflow.log_metric(\"roc_auc\",best_auc)\n",
        "\n",
        "  mlflow.spark.log_model(\n",
        "      best_model,\n",
        "      artifact_path=\"sentiment_model\"\n",
        "  )\n",
        "\n",
        "print(\"best model logged to mlflow:\",best_model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny88eee9YDJ7",
        "outputId": "2f6923e6-ed83-41d1-c05f-327fc992ebd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best model logged to mlflow: logistic_regression_cv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"/content/stream_input\", exist_ok=True)"
      ],
      "metadata": {
        "id": "81A8akWBsPCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "stream_df = spark.readStream.format(\"text\").load(\"/content/stream_input\")\n",
        "\n",
        "stream_df = stream_df.withColumnRenamed(\"value\",\"tweet\")"
      ],
      "metadata": {
        "id": "LYK6hjOlZq0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.functions import vector_to_array\n",
        "stream_df = clean_text_df(stream_df)\n",
        "\n",
        "stream_predictions = best_model.transform(stream_df).withColumn(\"prob_array\", vector_to_array(\"probability\"))\\\n",
        ".withColumn(\"positive_prob\", col(\"prob_array\")[1]).select(\"tweet\",\"prediction\",\"positive_prob\")"
      ],
      "metadata": {
        "id": "ppAp66lVbFdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = stream_predictions.writeStream.format(\"memory\").queryName(\"sentiment_stream\").outputMode(\"append\").start()"
      ],
      "metadata": {
        "id": "4xM7Y_Y9be8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Is stream active?\", query.isActive)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15gxETN8ukTz",
        "outputId": "33238dbe-a8f8-483e-a2b8-9dd1faebb91d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is stream active? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/stream_input/tweets1.txt\", \"w\") as f:\n",
        "    f.write(\"I love Spark streaming!\\n\")\n",
        "    f.write(\"This movie was terrible\\n\")"
      ],
      "metadata": {
        "id": "SvbM3odbtuca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/stream_input/tweets2.txt\", \"w\") as f:\n",
        "    f.write(\"I love Spark streaming\\n\")\n",
        "    f.write(\"This is the worst movie ever\\n\")"
      ],
      "metadata": {
        "id": "DvzvkCibvcIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/stream_input/tweets_101.txt\", \"w\") as f:\n",
        "    f.write(\"I love this product so much\\n\")\n",
        "    f.write(\"This is horrible and disappointing\\n\")"
      ],
      "metadata": {
        "id": "xQttiNKTwlDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM sentiment_stream\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIUk6w23wsYi",
        "outputId": "34376e11-d5b2-46c7-b5c1-8ba2f5a33b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------+----------+-------------------+\n",
            "|tweet                             |prediction|positive_prob      |\n",
            "+----------------------------------+----------+-------------------+\n",
            "|I love Spark streaming            |1.0       |0.734244627571385  |\n",
            "|This is the worst movie ever      |0.0       |0.30183064616507205|\n",
            "|I love Spark streaming!           |1.0       |0.734244627571385  |\n",
            "|This movie was terrible           |0.0       |0.29626714471245286|\n",
            "|I love this product so much       |1.0       |0.7707708784224886 |\n",
            "|This is horrible and disappointing|0.0       |0.02016066995660326|\n",
            "+----------------------------------+----------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "2prU_q8jvPqI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}